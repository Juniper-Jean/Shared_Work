#!/usr/bin/env python3

"""
Processes synthetic genetic data, which will be used to train a binary classifier.

When run as a standalone program, this script processes multiple datasets, each 
set sequentially, one after the other.

The `main` function:
- Takes as input a run number - a unique, sequential identifier for each 
experimental run (set of simulations).
- Retrieves a JSON configuration file corresponding to the current run number. 
This contains details/metadata about the run, e.g., parameters & output 
directory (a record to link parameter sets to outputs).
- Retrieves the directory path where data are stored for the run from the 
configuration file.

[
The script is part of a bigger workflow. Overview of workflow:
Base analysis:
- Training data - run a set of many simulations to generate (synthetic) data 
used to train a machine-learning (ML) model.
- The script 'generate_dataset.sh' runs the simulations & takes as input a .txt 
file of parameter values.
- Train a ML model on the synthetic data.

Power analysis:
- extends base analysis
- Vary values of some simulation parameters to assess the ML model's performance 
across different scenarios.
- Re-run simulations multiple times using a different set of parameter values 
each time.
- Train a new ML model on each unique set of training data.
- Explore the influence of each parameter & their interactions on the ML model's 
performance.

'An experimental run' (model run / one run) refers to one execution/instance of 
simulations using a parameter set & training one model (one cycle of training a 
ML model on data generated from a unique set of simulation parameters & then 
evaluating the model).
]
"""

__author__ = 'cpenning@ic.ac.uk'
__version__ = '0.0.2' # 2024 Feb 8

#-----
# Imports
#-----
# Standard-Library Imports
import os
# Module provides way to use functionality dependent on operating system.
# Incs fns to interact w file system in platform-independent way.

import json # module for working w/ JSON data

import pdb
# interactive debugger module- allows you to pause execution, inspect variables, 
# step through code, & evaluate expressions at runtime

import time
# module provides fns for working w/ times & dates
# It's essential for measuring performance / execution time, as it allows us to 
# capture precise time pts before & after code execution.

# Local-Application Imports
from ImaGene import *
# W/ ImaGene.py in same directory as this script, import everything from 
# 'ImaGene' module.


def process_simulations_binary(path_sim):
    """
    Process batches of synthetic data, which will be used to train a binary classifier.
    
    Handles a set of synthetic genetic data, previously generated by simulations. 
    The dataset is organised into multiple batches & this function processes 
    data batch by batch. Each batch undergoes various preprocessing steps, inc 
    filtering based on minor allele frequency, resizing images for uniformity, 
    flipping pixel values for a standard representation in genomic studies, 
    randomly shuffling images to avoid order bias, & converting target values to 
    binary format.
    
    Parameters:
    path_sim (str): directory path where output data of simulations are stored.
    """
    
    # pdb.set_trace() # debugger entry point
    
    for i in range(1, 11):
    # We split ea set of synthetic data into batches, so, later, we can train a 
    # neural network with a 'simulation-on-the-fly' approach.
    # Loop over nrs 1 to 10 inclusive- iterate over ea batch.
        
        
        #----
        # Load batch of synthetic data.
        #----
        print(f'Processing batch: {path_sim}/Simulations{i}/')
        # Output specified message to the console.
        # Identifies param set, replicate nr, & batch nr of data currently being processed.
        # f-string allows for dynamic insertion of var into string.
        
        file_sim = ImaFile(simulations_folder=os.path.join(path_sim, f'Simulations{i}'), 
                           nr_samples=198, model_name='Marth-3epoch-CEU')
        # Make `ImaFile` obj (initiate instance of `ImaFile` class) to access 
        # specific batch of sims.
        # `ImaFile` obj holds metadata & path to synthetic data (it doesn't 
        # actually load synthetic data, but acts as interface to access/manage it).
        # Use `os.path.join()` fn to construct path to specific batch of sims. 
        # Combine `path_sim` var (dir containing all batches of sim data) w/ 
        # dynamically generated dir name, `Simulations{i}`, where `i` is batch nr.
        
        # pdb.set_trace()
        gene_sim = file_sim.read_simulations(parameter_name='selection_coeff_hetero', 
                                             max_nrepl=2000) # *
                                            #  max_nrepl=20000) X
        # Load synthetic data into `ImaGene` obj (call `read_simulations` method 
        # of `ImaFile` instance).
        # Specify var we want to estimate/predict (feature of interest in sims- 
        # selection coefficient for heterozygotes, `selection_coeff_hetero`).
        # Set upper limit on nr of data pts (replicates) to load per class 
        # within sims. We may limit nr of data pts, eg to 2000 per class, as 
        # quick test example. This is useful if dealing w/ big dataset, as it 
        # keeps data handling efficient & manageable.
        
        gene_sim.summary()
        # Print overview of data stored in obj, inc nr of images it contains & 
        # stat info about dimensions of images (max, min, mean, & SD of rows & cols).
        # `.summary()`- method of `ImaGene` class called on `gene_sim` obj
        
        # gene_sim.plot()
        # `plot()` displays image from `ImaGene` obj, by default 1st image 
        # (index=0) as grayscale plot.
        
        
        #----
        # Process synthetic data.
        #----
        gene_sim.filter_freq(0.01)
        # gene_sim.summary()
        # gene_sim.plot()
        # Remove genetic variants (SNPs) w/ minor allele frequency (MAF) <1% 
        # -removes monomorphic sites, singletons, & other rare variants w/ MAFs <1%.
        # This filtering is standard practice in genomic studies.
        # It simplifies dataset by focusing on more common variants, which are 
        # generally more informative & less likely to be noise or sequencing errors.
        
        
        # gene_LCT.sort?
        # `gene_LCT.sort?` or `?gene_LCT.sort`:In IPython terminal & Jupyter 
        # notebooks, question mark before/after obj, fn, or method is used to 
        # display docstring or documentation related to that object.
        
        gene_sim.sort('rows_freq')
        # gene_sim.summary()
        # gene_sim.plot()
        # `.sort()` method of `ImaGene` class rearranges data within ea genomic 
        # image based on specified criterion.
        # It operates on ea image individually.
        # Sorting structures data in meaningful way, potentially making it easier 
        # for ML models to identify/learn genetic patterns.
        
        
        # gene_sim.resize? # See different options for resizing.
        gene_sim.resize((198, 192))
        # gene_sim.summary()
        # gene_sim.plot()
        # Resize all images in `ImaGene` obj to uniform dimension.
        # `ImaGene` tut, '01_binary.ipynb', resized all images to have shape 
        # (198, 192) (198 rows, 192 cols) to match dims of real data used in analysis.
        # In context of this power analysis w/o real data, we retain these dims 
        # for simplicity & to maintain consistency.
        # If deploying model on real genomic data, it's essential to adjust 
        # `.resize()` dims for training data to match dims of real data.
        # This ensures compatibility & accuracy in model's application to real datasets.
        
        # pdb.set_trace()
        gene_sim.convert(flip=True)
        # gene_sim.summary()
        # gene_sim.plot()
        # Use `.convert` method of `ImaGene` obj w/ `flip=True` keyword arg.
        # Converts images to proper numpy float matrices- from `uint8` format 
        # (integer vals from 0 to 255) to `float32` format (floating-point nrs).
        # Additionally, `flip=True` keyword arg reverses pixel vals to assign 
        # black to the alternate allele.
        # This ensures data is in format suitable for ML alg & enhances clarity 
        # of genetic patterns.
        # Flipping aligns w standard representation of genomic data where minor 
        # alleles are often marked distinctly for better interpretability.
        
        
        gene_sim.subset(get_index_random(gene_sim))
        # gene_sim.summary()
        # gene_sim.plot()
        # Randomise order of genomic images in `gene_sim` obj.
        # This is crucial for training ML models, as it prevents model from 
        # learning any order-specific biases & helps in generalising better to 
        # new, unseen data.
        # `gene_sim.subset(get_index_random(gene_sim))` randomly reorders 
        # collection of genomic images in entire dataset, but doesn't alter data 
        # within ea indiv image.
        
        # `gene_sim.subset(get_index_random(gene_sim))` 1st calls `get_index_random(gene_sim)` 
        # on `ImaGene` obj to generate randomly ordered array of indices corresponding 
        # to genomic images in `gene_sim`.
        # `gene_sim.subset(...)` then rearranges images based on this random sequence.
        # `get_index_random` fn in `ImaGene.py` module
        # `ImaGene.subset()` method (of `ImaGene` class)
        
        
        gene_sim.targets = to_binary(gene_sim.targets)
        # Convert target vals in `gene_sim` to binary format suitable for binary 
        # classification in Keras.
        # `targets` attribute of `ImaGene` class is array that holds target vals 
        # (classes/labels) for ea genomic image in (`gene_sim`) dataset.
        # `to_binary` fn in `ImaGene.py` module compares ea target val to min 
        # val in `targets` array (`targets.min()`). Fn sets target val to 0 if 
        # it equals min val; otherwise, it's set to 1.
        # After transformation, `targets` is numpy array of `float32` data type 
        # & contains only 0s & 1s- simplified binary classification problem, 
        # where ea val indicates binary class of corresponding image.
        # Conversion is necesscary to align w compatability requirements of 
        # binary classification tasks in Keras.
        
        # gene_sim.save(file=f'{path_sim}/gene_sim.binary')
        gene_sim.save(file=os.path.join(path_sim, 
                                        # f'Simulations{i}', 
                                        f'gene_sim_Batch{i}.binary'))
        # `gene_sim` obj is now ready for model training.
        # Use `.save()` method of `ImaGene` class to save it.
        # The method uses `pickle` module to serialise obj & save it in binary format.
        # Construct file path- use f-string to insert `path_sim` var directly 
        # into str.
        
        # gene_sim = load_imagene(file=f'{path_sim}/gene_sim.binary')
        # `load_imagene` fn in `ImaGene.py` module loads previously saved 
        # `ImaGene` obj.
        # It uses `pickle` module for deserialisation.


def main(analysis_version, run_nr):
    """
    Orchestrates execution of the script's primary task.
    
    Parameters:
    - analysis_version (str): version number of the analysis, used to construct 
    dir paths
    - run_nr (int): unique, sequential identifier for each experimental run (job).
    """
    
    config_file_path = os.path.join(analysis_version, 'Config_Files', 
                                    f'config{run_nr}.json')
    # Construct path to config file for current run nr.
    
    with open(config_file_path, 'r') as file:
    # Load config data- open config file in read mode.
        
        config_data = json.load(file)
        # Read JSON content from file & convert it into Python dict.
    
    path_sim = os.path.join('..', 'Data', analysis_version, 
                            config_data["run_output_dir"])
    # Construct path to dir where output data of sims are stored (synthetic 
    # genetic data for current run).
    # This is also dir where we save processed data.
    
    print(f'Processing synthetic data for run number: {run_nr} in {path_sim}')
    # Output specified message to the console.
    # Use f-string to dynamically insert `run_nr` & `path_sim` vars into str.
    # Provides feedback about which run is currently in progress.
    # Gives user way to track progress when running multiple runs sequentially.

    process_simulations_binary(path_sim)
    # Call fn to process synthetic data.


if __name__ == '__main__':
# Check if script is executed as standalone (main) program & call main fn if `True`.
    
    start_time = time.time()
    # start time
    # We'll use this to calculate script's total execution time by subtracting 
    # it from end time.
    
    analysis_version = 'Version1'
    # analysis_version = 'Version2'
    # nr_runs = 3
    nr_runs = 12
    # Specify version nr of analysis & nr of runs.
    # Adjust as necessary.
    
    for i in range(1, nr_runs + 1): # Iterate from 1 to `nr_runs` inclusive.

        run_start_time = time.time()
        # Capture current time at start of individual experimental run (job).

        main(analysis_version, i)
        # For ea run, call `main` fn, passing `analysis version` & current run 
        # nr as args.

        run_end_time = time.time() # end time for this run
        print(f'Execution time for run {i}: {run_end_time - run_start_time} s')
        # Calculate execution time for current run & output it to the console- 
        # show how long ea run took.
    
    end_time = time.time() # end time
    print(f'Total execution time: {end_time - start_time:.2f} s')
    # Subtract `start_time` from `end_time` to get duration of script execution in s.
    # Output duration to the console- show how long script took to run in total.
    # `:.2f`: a format specifier (syntax is specific to f-strings). Formats 
    # resulting floating-point nr to 2 decimal places for readability.

# Code block executes script's main task in sequential manner for specified nr 
# of runs.
# This is suitable for running on a local computer (as opposed to parallel 
# execution of task on high-performance computing cluster).
# Additionally, the dunder name main check enables us to execute script as 
# standalone (main) program. This allows us to prototype/test individual 
# components of workflow in isolation (for development purposes), ensuring they 
# work as expected before integrating them into bigger workflow.
